{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "from IPython.display import display, clear_output\n",
    "import PIL.Image\n",
    "import time\n",
    "import imageio\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "from nets.DDPGAgent import DDPGAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamoz\u001b[0m (\u001b[33marashmozhdehi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arash/MountainCarContinuous-Gym-RL/DDPG/wandb/run-20240712_233314-buouoq12</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arashmozhdehi/DDPG-MountainCarContinuous/runs/buouoq12' target=\"_blank\">colorful-rain-2</a></strong> to <a href='https://wandb.ai/arashmozhdehi/DDPG-MountainCarContinuous' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arashmozhdehi/DDPG-MountainCarContinuous' target=\"_blank\">https://wandb.ai/arashmozhdehi/DDPG-MountainCarContinuous</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arashmozhdehi/DDPG-MountainCarContinuous/runs/buouoq12' target=\"_blank\">https://wandb.ai/arashmozhdehi/DDPG-MountainCarContinuous/runs/buouoq12</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"DDPG-MountainCarContinuous\", config={\n",
    "    \"episodes\": 1000,\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": int(1e5),\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 1e-3,\n",
    "    \"actor_lr\": 1e-4,\n",
    "    \"critic_lr\": 1e-3,\n",
    "    \"weight_decay\": 0,\n",
    "    \"state_size\": 3,    # Assuming state size and action size are constants, you can also make them configurable\n",
    "    \"action_size\": 1,\n",
    "    \"random_seed\": 42\n",
    "})\n",
    "\n",
    "# Start an MLflow run\n",
    "mlflow.start_run()\n",
    "mlflow.log_params(wandb.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('videos', exist_ok=True)\n",
    "os.makedirs('check_points', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    random_seed=wandb.config.random_seed,\n",
    "    batch_size=wandb.config.batch_size,\n",
    "    buffer_size=wandb.config.buffer_size,\n",
    "    gamma=wandb.config.gamma,\n",
    "    tau=wandb.config.tau,\n",
    "    actor_lr=wandb.config.actor_lr,\n",
    "    critic_lr=wandb.config.critic_lr,\n",
    "    weight_decay=wandb.config.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames, path):\n",
    "    with imageio.get_writer(path, mode='I', fps=30) as writer:\n",
    "        for frame in frames:\n",
    "            display(PIL.Image.fromarray(frame))\n",
    "            clear_output(wait=True)\n",
    "            writer.append_data(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(agent, path):\n",
    "    if os.path.exists(path):\n",
    "        agent.load_checkpoint(path)\n",
    "        print(\"Loaded checkpoint from:\", path)\n",
    "    else:\n",
    "        print(\"No checkpoint found at:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Now you can specify whether to start from a checkpoint\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mddpg\u001b[0;34m(n_episodes, max_t, print_every, save_every, start_from_checkpoint)\u001b[0m\n\u001b[1;32m     24\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     25\u001b[0m next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(next_state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 26\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     28\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/MountainCarContinuous-Gym-RL/DDPG/nets/DDPGAgent.py:78\u001b[0m, in \u001b[0;36mDDPGAgent.step\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Check if enough samples are available in the memory for learning\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m---> 78\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(experiences)\n",
      "File \u001b[0;32m~/MountainCarContinuous-Gym-RL/DDPG/utils/Memory.py:47\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Randomly sample a batch of experiences from memory.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m        tuple: Batch of experiences including states, actions, rewards, next_states, and dones.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack([e\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     50\u001b[0m     actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack([e\u001b[38;5;241m.\u001b[39maction \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/usr/lib/python3.10/random.py:499\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    497\u001b[0m selected_add \u001b[38;5;241m=\u001b[39m selected\u001b[38;5;241m.\u001b[39madd\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n\u001b[0;32m--> 499\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[43mrandbelow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m selected:\n\u001b[1;32m    501\u001b[0m         j \u001b[38;5;241m=\u001b[39m randbelow(n)\n",
      "File \u001b[0;32m/usr/lib/python3.10/random.py:245\u001b[0m, in \u001b[0;36mRandom._randbelow_with_getrandbits\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    244\u001b[0m getrandbits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetrandbits\n\u001b[0;32m--> 245\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbit_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# don't use (n-1) here because n can be 1\u001b[39;00m\n\u001b[1;32m    246\u001b[0m r \u001b[38;5;241m=\u001b[39m getrandbits(k)  \u001b[38;5;66;03m# 0 <= r < 2**k\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m r \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "def ddpg(n_episodes=3000, max_t=300, print_every=100, save_every=500, start_from_checkpoint=False):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    \n",
    "    # Determine the starting episode based on the checkpoint\n",
    "    last_checkpoint_episode = 0\n",
    "    if start_from_checkpoint:\n",
    "        checkpoint_files = [f for f in os.listdir('check_points') if f.endswith('.pth')]\n",
    "        if checkpoint_files:\n",
    "            last_checkpoint = max(checkpoint_files, key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "            last_checkpoint_episode = int(last_checkpoint.split('_')[1].split('.')[0])\n",
    "            load_checkpoint(agent, os.path.join('check_points', last_checkpoint))\n",
    "\n",
    "    for i_episode in range(last_checkpoint_episode + 1, n_episodes + 1):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        frames = []\n",
    "        for t in range(max_t):\n",
    "            if isinstance(state, tuple):\n",
    "                state = np.array(state[0])\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = np.array(next_state, dtype=np.float32)\n",
    "            agent.step(state, action, reward, next_state, terminated or truncated)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            # Capture the frame\n",
    "            frame = env.render()  # Ensure to get RGB frames\n",
    "            frames.append(frame)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        wandb.log({\"Average Score\": np.mean(scores_deque)})\n",
    "        mlflow.log_metric(\"Score\", score, step=i_episode)\n",
    "\n",
    "        # Save the videos for the episode\n",
    "        videos_path = os.path.join('videos', f'videos_episode_{i_episode}.gif')\n",
    "        display_frames_as_gif(frames, videos_path)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        if i_episode % save_every == 0:\n",
    "            checkpoint_path = os.path.join('check_points', f'checkpoint_{i_episode}.pth')\n",
    "            agent.save_checkpoint(checkpoint_path)\n",
    "        \n",
    "    mlflow.end_run()\n",
    "    return scores\n",
    "\n",
    "# Now you can specify whether to start from a checkpoint\n",
    "scores = ddpg(start_from_checkpoint=True)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
