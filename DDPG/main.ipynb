{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "from IPython.display import display, clear_output\n",
    "import PIL.Image\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from nets.DDPGAgent import DDPGAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamoz\u001b[0m (\u001b[33marashmozhdehi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arash/MountainCarContinuous-Gym-RL/DDPG/wandb/run-20240712_201636-jfe07b7s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arashmozhdehi/DDPG-Pendulum/runs/jfe07b7s' target=\"_blank\">fancy-forest-11</a></strong> to <a href='https://wandb.ai/arashmozhdehi/DDPG-Pendulum' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arashmozhdehi/DDPG-Pendulum' target=\"_blank\">https://wandb.ai/arashmozhdehi/DDPG-Pendulum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arashmozhdehi/DDPG-Pendulum/runs/jfe07b7s' target=\"_blank\">https://wandb.ai/arashmozhdehi/DDPG-Pendulum/runs/jfe07b7s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"DDPG-Pendulum\", config={\n",
    "    \"episodes\": 1000,\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": int(1e5),\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 1e-3,\n",
    "    \"actor_lr\": 1e-4,\n",
    "    \"critic_lr\": 1e-3,\n",
    "    \"weight_decay\": 0,\n",
    "    \"state_size\": 3,    # Assuming state size and action size are constants, you can also make them configurable\n",
    "    \"action_size\": 1,\n",
    "    \"random_seed\": 42\n",
    "})\n",
    "\n",
    "# Start an MLflow run\n",
    "mlflow.start_run()\n",
    "mlflow.log_params(wandb.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    random_seed=wandb.config.random_seed,\n",
    "    batch_size=wandb.config.batch_size,\n",
    "    buffer_size=wandb.config.buffer_size,\n",
    "    gamma=wandb.config.gamma,\n",
    "    tau=wandb.config.tau,\n",
    "    actor_lr=wandb.config.actor_lr,\n",
    "    critic_lr=wandb.config.critic_lr,\n",
    "    weight_decay=wandb.config.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames, title):\n",
    "    for frame in frames:\n",
    "        display(PIL.Image.fromarray(frame))\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m         clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Use the refactored function\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36mddpg\u001b[0;34m(n_episodes, max_t, print_every)\u001b[0m\n\u001b[1;32m     15\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     16\u001b[0m next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(next_state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 17\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     19\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/MountainCarContinuous-Gym-RL/DDPG/nets/DDPGAgent.py:79\u001b[0m, in \u001b[0;36mDDPGAgent.step\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m     78\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MountainCarContinuous-Gym-RL/DDPG/nets/DDPGAgent.py:154\u001b[0m, in \u001b[0;36mDDPGAgent.train\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m    151\u001b[0m Q_targets_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target(next_states, actions_next)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Calculate Q targets for current states (y_i = r + gamma * Q'(s', a'))\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m Q_targets \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m Q_targets_next \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m))\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Compute the critic loss using Mean Squared Error\u001b[39;00m\n\u001b[1;32m    157\u001b[0m Q_expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_local(states, actions)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:38\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f, assigned\u001b[38;5;241m=\u001b[39massigned)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_torch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def ddpg(n_episodes=3000, max_t=300, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        frames = []\n",
    "        for t in range(max_t):\n",
    "            if isinstance(state, tuple):\n",
    "                state = np.array(state[0])\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = np.array(next_state, dtype=np.float32)\n",
    "            agent.step(state, action, reward, next_state, terminated or truncated)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            # Capture the frame\n",
    "            frame = env.render()  # Get the current frame\n",
    "            frames.append(frame)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        wandb.log({\"Average Score\": np.mean(scores_deque)})\n",
    "        mlflow.log_metric(\"Score\", score, step=i_episode)\n",
    "\n",
    "        # Display the captured frames as an animation\n",
    "        # if i_episode % print_every == 0 or i_episode == 1:\n",
    "        display_frames_as_gif(frames, f'Episode {i_episode}')\n",
    "\n",
    "    mlflow.end_run()\n",
    "    return scores\n",
    "\n",
    "# Use the refactored function\n",
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
