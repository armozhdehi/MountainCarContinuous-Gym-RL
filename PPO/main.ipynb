{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "from IPython.display import display, clear_output\n",
    "import PIL.Image\n",
    "import time\n",
    "import imageio\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "from nets.PPOAgent import PPOAgent\n",
    "from utils.Memory import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamoz\u001b[0m (\u001b[33marashmozhdehi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arash/MountainCarContinuous-Gym-RL/PPO/wandb/run-20240713_171950-d3knhij0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arashmozhdehi/PPO-MountainCarContinuous/runs/d3knhij0' target=\"_blank\">polar-voice-15</a></strong> to <a href='https://wandb.ai/arashmozhdehi/PPO-MountainCarContinuous' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arashmozhdehi/PPO-MountainCarContinuous' target=\"_blank\">https://wandb.ai/arashmozhdehi/PPO-MountainCarContinuous</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arashmozhdehi/PPO-MountainCarContinuous/runs/d3knhij0' target=\"_blank\">https://wandb.ai/arashmozhdehi/PPO-MountainCarContinuous/runs/d3knhij0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "wandb.init(project=\"PPO-MountainCarContinuous\", config={\n",
    "    \"episodes\": 1000,\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": int(1e5),\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 0.95,\n",
    "    \"actor_lr\": 1e-4,\n",
    "    \"critic_lr\": 1e-3,\n",
    "    \"weight_decay\": 0,\n",
    "    \"clip_epsilon\": 0.2,\n",
    "    \"update_steps\": 4,\n",
    "    \"hidden_size\": 256,  # Add hidden_size parameter\n",
    "    \"state_size\": state_size,\n",
    "    \"action_size\": action_size,\n",
    "    \"random_seed\": 42\n",
    "})\n",
    "\n",
    "# Start an MLflow run\n",
    "mlflow.start_run()\n",
    "mlflow.log_params(wandb.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config\n",
    "\n",
    "# Initialize the PPO agent\n",
    "agent = PPOAgent(state_size, action_size, seed=config.random_seed, hidden_size=config.hidden_size,\n",
    "                    lr=config.actor_lr, gamma=config.gamma, tau=config.tau,\n",
    "                    clip_epsilon=config.clip_epsilon, update_steps=config.update_steps)\n",
    "memory = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('videos', exist_ok=True)\n",
    "os.makedirs('check_points', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames, path):\n",
    "    with imageio.get_writer(path, mode='I', fps=30) as writer:\n",
    "        for frame in frames:\n",
    "            display(PIL.Image.fromarray(frame))\n",
    "            clear_output(wait=True)\n",
    "            writer.append_data(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(agent, path):\n",
    "    if os.path.exists(path):\n",
    "        agent.load_checkpoint(path)\n",
    "        print(\"Loaded checkpoint from:\", path)\n",
    "    else:\n",
    "        print(\"No checkpoint found at:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arash/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/home/arash/MountainCarContinuous-Gym-RL/PPO/nets/PPOAgent.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  states = torch.FloatTensor(states).to(device)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Value is not broadcastable with batch_shape+event_shape: torch.Size([200, 1]) vs torch.Size([201, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Now you can specify whether to start from a checkpoint\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mppo\u001b[0;34m(n_episodes, max_t, print_every, save_every, start_from_checkpoint)\u001b[0m\n\u001b[1;32m     37\u001b[0m advantages \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mcompute_gae(rewards, values, dones)\n\u001b[1;32m     38\u001b[0m returns \u001b[38;5;241m=\u001b[39m advantages \u001b[38;5;241m+\u001b[39m values[:\u001b[38;5;28mlen\u001b[39m(advantages)]\n\u001b[0;32m---> 39\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m memory\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     42\u001b[0m scores_deque\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[0;32m~/MountainCarContinuous-Gym-RL/PPO/nets/PPOAgent.py:47\u001b[0m, in \u001b[0;36mPPOAgent.update\u001b[0;34m(self, states, actions, log_probs, returns, advantages)\u001b[0m\n\u001b[1;32m     44\u001b[0m advantages \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(advantages)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_steps):\n\u001b[0;32m---> 47\u001b[0m     new_log_probs, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     ratio \u001b[38;5;241m=\u001b[39m (new_log_probs \u001b[38;5;241m-\u001b[39m old_log_probs)\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m     49\u001b[0m     surr1 \u001b[38;5;241m=\u001b[39m ratio \u001b[38;5;241m*\u001b[39m advantages\n",
      "File \u001b[0;32m~/MountainCarContinuous-Gym-RL/PPO/nets/ActorNet.py:76\u001b[0m, in \u001b[0;36mActorNet.evaluate_actions\u001b[0;34m(self, states, actions)\u001b[0m\n\u001b[1;32m     74\u001b[0m mu, std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(states)\n\u001b[1;32m     75\u001b[0m dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(mu, std)\n\u001b[0;32m---> 76\u001b[0m action_log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     77\u001b[0m dist_entropy \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mentropy()\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action_log_probs, dist_entropy\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/normal.py:79\u001b[0m, in \u001b[0;36mNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# compute the variance\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/distribution.py:297\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(actual_shape), \u001b[38;5;28mreversed\u001b[39m(expected_shape)):\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m j \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[0;32m--> 297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue is not broadcastable with batch_shape+event_shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m         )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     support \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport\n",
      "\u001b[0;31mValueError\u001b[0m: Value is not broadcastable with batch_shape+event_shape: torch.Size([200, 1]) vs torch.Size([201, 1])."
     ]
    }
   ],
   "source": [
    "def ppo(n_episodes=1000, max_t=200, print_every=100, save_every=500, start_from_checkpoint=False):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    last_checkpoint_episode = 0\n",
    "    if start_from_checkpoint:\n",
    "        checkpoint_files = [f for f in os.listdir('check_points') if f.endswith('.pth')]\n",
    "        if checkpoint_files:\n",
    "            last_checkpoint = max(checkpoint_files, key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "            last_checkpoint_episode = int(last_checkpoint.split('_')[1].split('.')[0])\n",
    "            agent.load_checkpoint(os.path.join('check_points', last_checkpoint))\n",
    "\n",
    "    for i_episode in range(last_checkpoint_episode + 1, n_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        frames = []\n",
    "        for t in range(max_t):\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "            action, action_log_prob = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = np.array(next_state, dtype=np.float32)\n",
    "            value = agent.critic(torch.FloatTensor(state).to(device).unsqueeze(0)).item()\n",
    "            memory.store(state, action, action_log_prob, reward, value, terminated or truncated)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            # Capture the frame\n",
    "            frame = env.render()  # Ensure to get RGB frames\n",
    "            frames.append(frame)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Calculate next value and update agent\n",
    "        next_value = agent.critic(torch.FloatTensor(next_state).to(device).unsqueeze(0)).item()\n",
    "        memory.store(next_state, None, None, None, next_value, terminated or truncated)\n",
    "        rewards = memory.rewards\n",
    "        values = memory.values\n",
    "        dones = memory.dones\n",
    "        advantages = agent.compute_gae(rewards, values, dones)\n",
    "        returns = advantages + values[:len(advantages)]\n",
    "        agent.update(memory.states, memory.actions, memory.log_probs, returns, advantages)\n",
    "        memory.clear()\n",
    "\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        wandb.log({\"Average Score\": np.mean(scores_deque)})\n",
    "        mlflow.log_metric(\"Score\", score, step=i_episode)\n",
    "\n",
    "        # Save the videos for the episode\n",
    "        videos_path = os.path.join('videos', f'videos_episode_{i_episode}.gif')\n",
    "        display_frames_as_gif(frames, videos_path)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        if i_episode % save_every == 0:\n",
    "            checkpoint_path = os.path.join('check_points', f'checkpoint_{i_episode}.pth')\n",
    "            agent.save_checkpoint(checkpoint_path)\n",
    "        \n",
    "    mlflow.end_run()\n",
    "    return scores\n",
    "\n",
    "# Now you can specify whether to start from a checkpoint\n",
    "scores = ppo(start_from_checkpoint=True)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
