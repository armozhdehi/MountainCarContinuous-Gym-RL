{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "from IPython.display import display, clear_output\n",
    "import PIL.Image\n",
    "import time\n",
    "import imageio\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "from nets.PPOAgent import PPOAgent\n",
    "from utils.Memory import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamoz\u001b[0m (\u001b[33marashmozhdehi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arash/MountainCarContinuous-Gym-RL/PPO/wandb/run-20240713_172851-gngbx3sv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arashmozhdehi/PPO-MountainCarContinuous/runs/gngbx3sv' target=\"_blank\">true-music-17</a></strong> to <a href='https://wandb.ai/arashmozhdehi/PPO-MountainCarContinuous' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arashmozhdehi/PPO-MountainCarContinuous' target=\"_blank\">https://wandb.ai/arashmozhdehi/PPO-MountainCarContinuous</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arashmozhdehi/PPO-MountainCarContinuous/runs/gngbx3sv' target=\"_blank\">https://wandb.ai/arashmozhdehi/PPO-MountainCarContinuous/runs/gngbx3sv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "wandb.init(project=\"PPO-MountainCarContinuous\", config={\n",
    "    \"episodes\": 1000,\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": int(1e5),\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 0.95,\n",
    "    \"actor_lr\": 1e-4,\n",
    "    \"critic_lr\": 1e-3,\n",
    "    \"weight_decay\": 0,\n",
    "    \"clip_epsilon\": 0.2,\n",
    "    \"update_steps\": 4,\n",
    "    \"hidden_size\": 256,  # Add hidden_size parameter\n",
    "    \"state_size\": state_size,\n",
    "    \"action_size\": action_size,\n",
    "    \"random_seed\": 42\n",
    "})\n",
    "\n",
    "# Start an MLflow run\n",
    "mlflow.start_run()\n",
    "mlflow.log_params(wandb.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config\n",
    "\n",
    "# Initialize the PPO agent\n",
    "agent = PPOAgent(state_size, action_size, seed=config.random_seed, hidden_size=config.hidden_size,\n",
    "                    lr=config.actor_lr, gamma=config.gamma, tau=config.tau,\n",
    "                    clip_epsilon=config.clip_epsilon, update_steps=config.update_steps)\n",
    "memory = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('videos', exist_ok=True)\n",
    "os.makedirs('check_points', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames, path):\n",
    "    with imageio.get_writer(path, mode='I', fps=30) as writer:\n",
    "        for frame in frames:\n",
    "            display(PIL.Image.fromarray(frame))\n",
    "            clear_output(wait=True)\n",
    "            writer.append_data(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(agent, path):\n",
    "    if os.path.exists(path):\n",
    "        agent.load_checkpoint(path)\n",
    "        print(\"Loaded checkpoint from:\", path)\n",
    "    else:\n",
    "        print(\"No checkpoint found at:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Now you can specify whether to start from a checkpoint\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m, in \u001b[0;36mppo\u001b[0;34m(n_episodes, max_t, print_every, save_every, start_from_checkpoint)\u001b[0m\n\u001b[1;32m     24\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Capture the frame\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure to get RGB frames\u001b[39;00m\n\u001b[1;32m     28\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/classic_control/continuous_mountain_car.py:291\u001b[0m, in \u001b[0;36mContinuous_MountainCarEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[0;32m--> 291\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurfarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixels3d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    292\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def ppo(n_episodes=1000, max_t=200, print_every=100, save_every=500, start_from_checkpoint=False):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    last_checkpoint_episode = 0\n",
    "    if start_from_checkpoint:\n",
    "        checkpoint_files = [f for f in os.listdir('check_points') if f.endswith('.pth')]\n",
    "        if checkpoint_files:\n",
    "            last_checkpoint = max(checkpoint_files, key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "            last_checkpoint_episode = int(last_checkpoint.split('_')[1].split('.')[0])\n",
    "            agent.load_checkpoint(os.path.join('check_points', last_checkpoint))\n",
    "\n",
    "    for i_episode in range(last_checkpoint_episode + 1, n_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        frames = []\n",
    "        for t in range(max_t):\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "            action, action_log_prob = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = np.array(next_state, dtype=np.float32)\n",
    "            value = agent.critic(torch.FloatTensor(state).to(device).unsqueeze(0)).item()\n",
    "            memory.store(state, action, action_log_prob, reward, value, terminated or truncated)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            # Capture the frame\n",
    "            frame = env.render()  # Ensure to get RGB frames\n",
    "            frames.append(frame)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Calculate next value and update agent\n",
    "        next_value = agent.critic(torch.FloatTensor(next_state).to(device).unsqueeze(0)).item()\n",
    "        rewards = memory.rewards\n",
    "        values = memory.values + [next_value]  # Append the next value at the end\n",
    "        dones = memory.dones\n",
    "        advantages = agent.compute_gae(rewards, values, dones)\n",
    "        returns = advantages + values[:-1]\n",
    "        agent.update(memory.states, memory.actions, memory.log_probs, returns, advantages)\n",
    "        memory.clear()\n",
    "\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        wandb.log({\"Average Score\": np.mean(scores_deque)})\n",
    "        mlflow.log_metric(\"Score\", score, step=i_episode)\n",
    "\n",
    "        # Save the videos for the episode\n",
    "        videos_path = os.path.join('videos', f'videos_episode_{i_episode}.gif')\n",
    "        display_frames_as_gif(frames, videos_path)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        if i_episode % save_every == 0:\n",
    "            checkpoint_path = os.path.join('check_points', f'checkpoint_{i_episode}.pth')\n",
    "            agent.save_checkpoint(checkpoint_path)\n",
    "        \n",
    "    mlflow.end_run()\n",
    "    return scores\n",
    "\n",
    "# Now you can specify whether to start from a checkpoint\n",
    "scores = ppo(start_from_checkpoint=True)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
